{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation for SOMOSPIE\n",
    "Soil moisture is a critical variable that links climate dynamics with water and food security. It regulates land-atmosphere interactions (e.g., via evapotranspiration--the loss of water from evaporation and plant transpiration to the atmosphere), and it is directly linked with plant productivity and survival. Information on soil moisture is important to design appropriate irrigation strategies to increase crop yield, and long-term soil moisture coupled with climate information provides insights into trends and potential agricultural thresholds and risks. Thus, information on soil moisture is a key factor to inform and enable precision agriculture.\n",
    "\n",
    "The current availability in soil moisture data over large areas comes from remote sensing (i.e., satellites with radar sensors) which provide daily, nearly global coverage of soil moisture. However, satellite soil moisture datasets have a major shortcoming in that they are limited to coarse spatial resolution (generally no finer than tens of kilometers).\n",
    "\n",
    "There do exist at higher resolution other geographic datasets (e.g., climatic, geological, and topographic) that are intimately related to soil moisture values. SOMOSPIE is meant to be a general-purpose tool for using such datasets to downscale (i.e., increase resolution) satelite-based soil moisture products. This Jupyter Notebook is a result of a collaboration between computer scientists of the Global Computing Laboratory at the Universtiy of Tennessee, Knoxville and soil scientists at the University of Delware (funded by NSF awards #1724843 and #1854312).\n",
    "\n",
    "This notebook combines terrain parameters withsatellite-based soil moisture data to generate the input files to the ML prediction workflow. \n",
    "\n",
    "## Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSN credentials and setup\n",
    "Before running the workflow, specify your access key and secret key in the Pegasus credentials file at ~/.pegasus/credentials.conf with the format below.\n",
    "\n",
    "```\n",
    "[osn]\n",
    "endpoint = https://sdsc.osn.xsede.org\n",
    "\n",
    "[USER@osn]\n",
    "access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "secret_key = abababababababababababababababab\n",
    "```\n",
    "**Note:** Replace USER with your ACCESS username\n",
    "\n",
    "In the following code cell also specify the OSN bucket and ACCESS username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update to a OSN bucket you have access to. For example asc190064-bucket01 \n",
    "osn_bucket=\"BUCKET\" \n",
    "# update to your ACCESS username\n",
    "access_user=\"USER\"\n",
    "\n",
    "!chmod 600 ~/.pegasus/credentials.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters\n",
    "In the code cell bellow specify the inputs to the workflow:\n",
    "\n",
    "* **year:** year from which to fetch soil moisture data.\n",
    "* **avg_type:** Averaging type over soil moisture values (monthly is the only option currently available).\n",
    "* **param_names:** Name of the terrain parameters that will be combined with the satellite data.\n",
    "* **param_paths:** Path to the terrain parameters that will be combined with the satellite data.\n",
    "* **shp_path:** Shp file in zip format of the region of interest.\n",
    "* **n_tiles:** Number of tiles of the elevation data both from the x and y axis, total number of tiles = n_tiles*n_tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2010\n",
    "avg_type = 'monthly'\n",
    "param_names = ['aspect', 'elevation', 'hillshading', 'slope']\n",
    "param_paths = [\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/TerrainParameters/OK_10m/\" + param + '.tif' for param in param_names]\n",
    "shp_path = \"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/shpFiles/OK.zip\"\n",
    "n_tiles = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus logging and properties\n",
    "Some properties for the workflow are specified, such as the data staging configuration to NonShared FileSystem to be able to use OSN for the intermediate and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "\n",
    "stg_out = False # Set to True to disable cleanup jobs on intermediate files\n",
    "\n",
    "# --- Properties ---------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"  \n",
    "# props[\"pegasus.mode\"] = \"tutorial\" # speeds up tutorial workflows - remove for production ones\n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "props[\"pegasus.data.configuration\"] = \"nonsharedfs\"\n",
    "props[\"pegasus.transfer.threads\"] = \"10\"\n",
    "props[\"pegasus.transfer.lite.threads\"] = \"10\"\n",
    "props[\"pegasus.integrity.checking\"] = \"none\" # temporary, bug\n",
    "#props[\"pegasus.transfer.bypass.input.staging\"] = \"true\"\n",
    "props.write() # written to ./pegasus.properties "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replica Catalog\n",
    "The input files to the workflow are specified in the Replica Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = ReplicaCatalog()\n",
    "\n",
    "param_files = []\n",
    "for param_path in param_paths:\n",
    "    param_files.append(File(os.path.basename(param_path)))\n",
    "    rc.add_replica(site=\"osn\", lfn=param_files[-1], pfn=param_path)\n",
    "\n",
    "shp_file = File(os.path.basename(shp_path))\n",
    "rc.add_replica(site=\"osn\", lfn=shp_file, pfn=shp_path)\n",
    "\n",
    "rc.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Catalog\n",
    "In this catalog the container in which the workflow will be run is specified along with the scripts that contain each of the functions of the workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Container ----------------------------------------------------------\n",
    "base_container = Container(\n",
    "                  \"base-container\",\n",
    "                  Container.SINGULARITY,\n",
    "                  image=\"docker://olayap/somospie-gdal-netcdf\")\n",
    "\n",
    "# --- Transformations ----------------------------------------------------------\n",
    "get_sm = Transformation(\n",
    "                \"get_sm.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/get_sm.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_memory=\"3GB\")\n",
    "\n",
    "generate_train = Transformation(\n",
    "                \"generate_train.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/generate_train.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_memory=\"3GB\")\n",
    "\n",
    "generate_eval = Transformation(\n",
    "                \"generate_eval.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/generate_eval.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_memory=\"8GB\", request_disk=\"70GB\")\n",
    "\n",
    "\n",
    "tc = TransformationCatalog()\\\n",
    "    .add_containers(base_container)\\\n",
    "    .add_transformations(get_sm, generate_train, generate_eval)\\\n",
    "    .write() # written to ./transformations.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site Catalog\n",
    "Specifies the OSN bucket where the files from the workflow will be stored and the local site where the input files and scripts are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Site Catalog ------------------------------------------------- \n",
    "osn = Site(\"osn\", arch=Arch.X86_64, os_type=OS.LINUX)\n",
    "\n",
    "# create and add a bucket in OSN to use for your workflows\n",
    "osn_shared_scratch_dir = Directory(Directory.SHARED_SCRATCH, path=\"/\" + osn_bucket + \"/DataTransformation/work\") \\\n",
    "    .add_file_servers(FileServer(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/DataTransformation/work\", Operation.ALL),)\n",
    "osn_shared_storage_dir = Directory(Directory.SHARED_STORAGE, path=\"/\" + osn_bucket + \"/DataTransformation/storage\") \\\n",
    "    .add_file_servers(FileServer(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/DataTransformation/storage\", Operation.ALL),)\n",
    "osn.add_directories(osn_shared_scratch_dir, osn_shared_storage_dir)\n",
    "\n",
    "# add a local site with an optional job env file to use for compute jobs\n",
    "shared_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "local_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "local = Site(\"local\") \\\n",
    "    .add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL)))\n",
    "\n",
    "#job_env_file = Path(str(BASE_DIR) + \"/../tools/job-env-setup.sh\").resolve()\n",
    "#local.add_pegasus_profile(pegasus_lite_env_source=job_env_file)\n",
    "\n",
    "#condorpool_site = Site(\"condorpool\")\n",
    "#condorpool_site.add_condor_profile(request_cpus=1, request_memory=\"9 GB\", request_disk=\"9 GB\")\n",
    "\n",
    "sc = SiteCatalog()\\\n",
    "   .add_sites(osn, local)\\\n",
    "   .write() # written to ./sites.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "The workflow is specified in the next code cell with the inputs, output and intermediate files. The latter also have specified cleanup jobs by using the argument **stage_out=False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"DataTransformation\")\n",
    "\n",
    "avg_files = [File('{0:02d}.tif'.format(month)) for month in range(1, 13)]\n",
    "\n",
    "job_get_sm = Job(get_sm)\\\n",
    "                .add_args(\"-y\", year, \"-a\", avg_type, \"-o\", *avg_files)\\\n",
    "                .add_outputs(*avg_files, stage_out=stg_out) # bypass_staging=False\n",
    "\n",
    "wf.add_jobs(job_get_sm)\n",
    "\n",
    "# Generate training files\n",
    "for i, avg_file in enumerate(avg_files):\n",
    "    train_file = File('{0:04d}_{1:02d}.tif'.format(year, i + 1))\n",
    "    train_file_aux = File('{0:04d}_{1:02d}.tif.aux.xml'.format(year, i + 1))\n",
    "    job_generate_train = Job(generate_train)\\\n",
    "                        .add_args(\"-i\", avg_file, \"-o\", train_file, \"-f\", *param_files, \"-p\", *param_names, \"-s\", shp_file)\\\n",
    "                        .add_inputs(avg_file, *param_files, shp_file)\\\n",
    "                        .add_outputs(train_file, train_file_aux, stage_out=True)\n",
    "    wf.add_jobs(job_generate_train)\n",
    "    \n",
    "# Generate eval files    \n",
    "if n_tiles == 0:\n",
    "    eval_file = File('eval.tif')\n",
    "    eval_file_aux = File(\"eval.tif.aux.xml\")\n",
    "    job_generate_eval = Job(generate_eval)\\\n",
    "                    .add_args(\"-i\", *param_files, \"-p\", *param_names, \"-n\", n_tiles, \"-s\", shp_file, \"-o\", eval_file)\\\n",
    "                    .add_inputs(*param_files, shp_file)\\\n",
    "                    .add_outputs(eval_file, eval_file_aux, stage_out=True)\n",
    "    wf.add_jobs(job_generate_eval)\n",
    "\n",
    "tile_count = 0\n",
    "for i in range(n_tiles):\n",
    "    for j in range(n_tiles):\n",
    "        eval_path = \"eval_{0:04d}.tif\".format(tile_count)\n",
    "        eval_file = File(eval_path)\n",
    "        eval_file_aux = File(eval_path + \".aux.xml\")\n",
    "        job_generate_eval = Job(generate_eval)\\\n",
    "                        .add_args(\"-i\", *param_files, \"-p\", *param_names, \"-n\", n_tiles, \"-x\", i, \"-y\", j, \"-s\", shp_file, \"-o\", eval_file)\\\n",
    "                        .add_inputs(*param_files, shp_file)\\\n",
    "                        .add_outputs(eval_file, eval_file_aux, stage_out=True)\n",
    "        wf.add_jobs(job_generate_eval)\n",
    "        \n",
    "        tile_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n",
    "\n",
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and submit the Workflow\n",
    "In this case OSN is specified for data staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(staging_sites={\"condorpool\": \"osn\"}, sites=[\"condorpool\"], output_sites=[\"osn\"], submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the workflow\n",
    "Pegasus returns statistics from the run of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug the workflow\n",
    "In case of failure `wf.analyze()` is helpful to find the cause of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.analyze()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
