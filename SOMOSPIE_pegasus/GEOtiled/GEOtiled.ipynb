{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEOtiled: A Scalable Workflow for Generating Large Datasets of High-Resolution Terrain Parameters\n",
    "\n",
    "Terrain parameters such as slope, aspect and hillshading can be computed from a Digital Elevation Model (DEM) which is a representation of elevation data of the surface of the earth. These parameters can be generated at different spatial resolutions and are fundamental in applications such as forestry and agriculture, hydrology, landscape ecology, land-atmosphere interactions, and soil moisture prediction.\n",
    "\n",
    "GEOtiled comprises three stages: (i) the partition of the DEM into tiles, each with a buffer region; (ii) the computation of the terrain parameters for each individual tile; and finally, (iii) the generation of a mosaic for each parameter from the tiles by averaging the values of the pixels that overlap between the tiles (i.e., pixels within the buffer regions). \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"../../somospie_pngs/geotiled.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<b>Figure 1: </b>GEOtiled workflow.\n",
    "</p>\n",
    "\n",
    "This notebook uses DEMs from [USGS 3DEP products](https://www.usgs.gov/3d-elevation-program/about-3dep-products-services) to compute 3 topographic parameters: Aspect, Hillshading and Slope. The parameters are returned as GeoTIFF files using EPSG:4326 projection.\n",
    "\n",
    "Before running the workflow on this notebook, go to [USGS Data Download Application](https://apps.nationalmap.gov/downloader/#/elevation) and use the map to look for available DEM data. Once you have selected a specific region and resolution, you can get a txt file with all the individual download links for the tiles corresponding to your selection. This txt file will serve as input to this notebook which uses the links to download the tiles and compute the parameters. This workflow works with GeoTIFF files.\n",
    "\n",
    "## Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters\n",
    "In the code cell bellow specify the inputs to the workflow:\n",
    "* **links_file:** path to the txt file with download links for DEM tiles you wish to use.\n",
    "* **projection:** path to a wkt file. To compute terrain parameters correctly, the DEM must be in a projection whose x, y and z coordinates are expressed in the same units, Albers Equal Area USGS projection was used for CONUS, but you can modify it depending on the region you are analyzing.\n",
    "* **n_tiles:** Number of tiles both from the x and y axis, total number of tiles = n_tiles*n_tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_file = 'OK_10m.txt'\n",
    "projection_file = 'projection.wkt'\n",
    "n_tiles = 2 # Number of tiles both from the x and y axis, total number of tiles = n_tiles*n_tiles\n",
    "\n",
    "# Read file with links\n",
    "with open(links_file, 'r', encoding='utf8') as f:\n",
    "        download_links = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSN credentials and setup\n",
    "Before running the workflow, specify your access key and secret key in the Pegasus credentials file at ~/.pegasus/credentials.conf with the format below.\n",
    "\n",
    "```\n",
    "[osn]\n",
    "endpoint = https://sdsc.osn.xsede.org\n",
    "\n",
    "[USER@osn]\n",
    "access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "secret_key = abababababababababababababababab\n",
    "```\n",
    "**Note:** Replace USER with your ACCESS username\n",
    "\n",
    "In the following code cell also specify the OSN bucket and ACCESS username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update to a OSN bucket you have access to. For example asc190064-bucket01 \n",
    "osn_bucket=\"BUCKET\" \n",
    "# update to your ACCESS username\n",
    "access_user=\"USER \"\n",
    "\n",
    "!chmod 600 ~/.pegasus/credentials.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus logging and properties\n",
    "Some properties for the workflow are specified, such as the data staging configuration to NonShared FileSystem to be able to use OSN for the intermediate and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "\n",
    "stg_out = False # Set to True to disable cleanup jobs on intermediate files\n",
    "\n",
    "# --- Properties ---------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"  \n",
    "# props[\"pegasus.mode\"] = \"tutorial\" # speeds up tutorial workflows - remove for production ones\n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "props[\"pegasus.data.configuration\"] = \"nonsharedfs\"\n",
    "props[\"pegasus.transfer.threads\"] = \"10\"\n",
    "props[\"pegasus.transfer.lite.threads\"] = \"10\"\n",
    "#props[\"pegasus.transfer.bypass.input.staging\"] = \"true\"\n",
    "props.write() # written to ./pegasus.properties "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replica Catalog\n",
    "The input files to the workflow are specified in the Replica Catalog, specifically the input tiles that Pegasus will download from USGS servers and the wkt file that contains the information about the projection in which the computation will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = ReplicaCatalog()\n",
    "input_tiles = []\n",
    "for f in download_links:\n",
    "    input_tiles.append(File(os.path.basename(f.strip())))\n",
    "    rc.add_replica(site=\"http\", lfn=input_tiles[-1], pfn=f)\n",
    "\n",
    "projection = File(projection_file)\n",
    "rc.add_replica(site=\"local\", lfn=projection, pfn=Path(\".\").resolve() / projection_file)\n",
    "\n",
    "rc.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Catalog\n",
    "In this catalog the container in which the workflow will be run is specified along with the scripts that contain each of the functions of the workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Container ----------------------------------------------------------\n",
    "base_container = Container(\n",
    "                  \"base-container\",\n",
    "                  Container.SINGULARITY,\n",
    "                  image=\"docker://olayap/somospie-gdal\")\n",
    "\n",
    "# --- Transformations ----------------------------------------------------------\n",
    "merge = Transformation(\n",
    "                \"merge.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/merge.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"50GB\", request_memory=\"5GB\")\n",
    "\n",
    "reproject = Transformation(\n",
    "                \"reproject.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/reproject.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"40GB\", request_memory=\"5GB\")\n",
    "\n",
    "crop = Transformation(\n",
    "                \"crop.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/crop.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"30GB\", request_memory=\"5GB\")\n",
    "\n",
    "compute = Transformation(\n",
    "                \"compute.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/compute.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"20GB\", request_memory=\"5GB\")\n",
    "\n",
    "merge_avg = Transformation(\n",
    "                \"merge_avg.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/merge_avg.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"50GB\", request_memory=\"10GB\") # For CONUS@10m 1TB, 20GB\n",
    "\n",
    "\n",
    "tc = TransformationCatalog()\\\n",
    "    .add_containers(base_container)\\\n",
    "    .add_transformations(merge, reproject, crop, compute, merge_avg)\\\n",
    "    .write() # written to ./transformations.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site Catalog\n",
    "Specifies the OSN bucket where the files from the workflow will be stored and the local site where the input files and scripts are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Site Catalog ------------------------------------------------- \n",
    "osn = Site(\"osn\", arch=Arch.X86_64, os_type=OS.LINUX)\n",
    "\n",
    "# create and add a bucket in OSN to use for your workflows\n",
    "osn_shared_scratch_dir = Directory(Directory.SHARED_SCRATCH, path=\"/\" + osn_bucket + \"/GEOtiled/work\") \\\n",
    "    .add_file_servers(FileServer(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/GEOtiled/work\", Operation.ALL),)\n",
    "osn_shared_storage_dir = Directory(Directory.SHARED_STORAGE, path=\"/\" + osn_bucket + \"/GEOtiled/storage\") \\\n",
    "    .add_file_servers(FileServer(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/GEOtiled/storage\", Operation.ALL),)\n",
    "osn.add_directories(osn_shared_scratch_dir, osn_shared_storage_dir)\n",
    "\n",
    "# add a local site with an optional job env file to use for compute jobs\n",
    "shared_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "local_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "local = Site(\"local\") \\\n",
    "    .add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL)))\n",
    "\n",
    "#job_env_file = Path(str(BASE_DIR) + \"/../tools/job-env-setup.sh\").resolve()\n",
    "#local.add_pegasus_profile(pegasus_lite_env_source=job_env_file)\n",
    "\n",
    "#condorpool_site = Site(\"condorpool\")\n",
    "#condorpool_site.add_condor_profile(request_cpus=1, request_memory=\"9 GB\", request_disk=\"9 GB\")\n",
    "\n",
    "sc = SiteCatalog()\\\n",
    "   .add_sites(osn, local)\\\n",
    "   .write() # written to ./sites.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "The workflow is specified in the next code cell with the inputs, output and intermediate files. The latter also have specified cleanup jobs by using the argument **stage_out=False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"GEOtiled\")\n",
    "\n",
    "mosaic = File(\"mosaic.tif\")\n",
    "job_merge = Job(merge)\\\n",
    "                .add_args(\"-i\", *input_tiles, \"-o\", mosaic)\\\n",
    "                .add_inputs(*input_tiles, bypass_staging=stg_out)\\\n",
    "                .add_outputs(mosaic, stage_out=stg_out)\n",
    "\n",
    "wf.add_jobs(job_merge)\n",
    "\n",
    "dem_m = File(\"elevation_m.tif\")\n",
    "job_reproject = Job(reproject)\\\n",
    "                    .add_args(\"-p\", projection,\"-i\", mosaic, \"-o\", dem_m)\\\n",
    "                    .add_inputs(mosaic, projection)\\\n",
    "                    .add_outputs(dem_m, stage_out=stg_out)\n",
    "    \n",
    "wf.add_jobs(job_reproject)\n",
    "\n",
    "# Crop tiles and compute each parameter for each tile\n",
    "aspect_tiles = []\n",
    "hillshading_tiles = []\n",
    "slope_tiles = []\n",
    "\n",
    "tile_count = 0\n",
    "for i in range(n_tiles):\n",
    "    for j in range(n_tiles):\n",
    "        tile = File(\"tile_{0:04d}.tif\".format(tile_count))\n",
    "        aspect_tiles.append(File(\"aspect_tile_{0:04d}.tif\".format(tile_count)))\n",
    "        hillshading_tiles.append(File(\"hillshading_tile_{0:04d}.tif\".format(tile_count)))\n",
    "        slope_tiles.append(File(\"slope_tile_{0:04d}.tif\".format(tile_count)))\n",
    "\n",
    "        job_crop = Job(crop)\\\n",
    "                            .add_args(\"-n\", n_tiles, \"-x\", i, \"-y\", j, \"-i\", dem_m, \"-o\", tile)\\\n",
    "                            .add_inputs(dem_m)\\\n",
    "                            .add_outputs(tile, stage_out=stg_out)\n",
    "        \n",
    "        job_compute = Job(compute)\\\n",
    "                            .add_args(\"-i\", tile, \"-o\", aspect_tiles[-1], hillshading_tiles[-1], slope_tiles[-1])\\\n",
    "                            .add_inputs(tile)\\\n",
    "                            .add_outputs(aspect_tiles[-1], hillshading_tiles[-1], slope_tiles[-1], stage_out=stg_out)\n",
    "\n",
    "        tile_count += 1\n",
    "        wf.add_jobs(job_crop, job_compute)\n",
    "        \n",
    "# DEM reprojected to WGS84\n",
    "dem = File(\"elevation.tif\")\n",
    "job_reproject = Job(reproject)\\\n",
    "                    .add_args(\"-p\", 'EPSG:4326',\"-i\", mosaic, \"-o\", dem, \"-n\", \"y\")\\\n",
    "                    .add_inputs(mosaic)\\\n",
    "                    .add_outputs(dem, stage_out=True)\n",
    "\n",
    "# Mosaic of each parameter\n",
    "aspect = File(\"aspect.tif\")\n",
    "job_avg0 = Job(merge_avg)\\\n",
    "                .add_args(\"-i\", *aspect_tiles, \"-o\", aspect)\\\n",
    "                .add_inputs(*aspect_tiles)\\\n",
    "                .add_outputs(aspect, stage_out=True)\n",
    "\n",
    "hillshading = File(\"hillshading.tif\")\n",
    "job_avg1 = Job(merge_avg)\\\n",
    "                .add_args(\"-i\", *hillshading_tiles, \"-o\", hillshading)\\\n",
    "                .add_inputs(*hillshading_tiles)\\\n",
    "                .add_outputs(hillshading, stage_out=True)\n",
    "\n",
    "slope = File(\"slope.tif\")\n",
    "job_avg2 = Job(merge_avg)\\\n",
    "                .add_args(\"-i\", *slope_tiles, \"-o\", slope)\\\n",
    "                .add_inputs(*slope_tiles)\\\n",
    "                .add_outputs(slope, stage_out=True)\n",
    "\n",
    "wf.add_jobs(job_reproject, job_avg0, job_avg1, job_avg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n",
    "\n",
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and submit the Workflow\n",
    "In this case OSN is specified for data staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(staging_sites={\"condorpool\": \"osn\"}, sites=[\"condorpool\"], output_sites=[\"osn\"], cluster=['horizontal'], submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the workflow\n",
    "Pegasus returns statistics from the run of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug the workflow\n",
    "In case of failure `wf.analyze()` is helpful to find the cause of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.analyze()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
