{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOMOSPIE\n",
    "Soil moisture is a critical variable that links climate dynamics with water and food security. It regulates land-atmosphere interactions (e.g., via evapotranspiration--the loss of water from evaporation and plant transpiration to the atmosphere), and it is directly linked with plant productivity and survival. Information on soil moisture is important to design appropriate irrigation strategies to increase crop yield, and long-term soil moisture coupled with climate information provides insights into trends and potential agricultural thresholds and risks. Thus, information on soil moisture is a key factor to inform and enable precision agriculture.\n",
    "\n",
    "The current availability in soil moisture data over large areas comes from remote sensing (i.e., satellites with radar sensors) which provide daily, nearly global coverage of soil moisture. However, satellite soil moisture datasets have a major shortcoming in that they are limited to coarse spatial resolution (generally no finer than tens of kilometers).\n",
    "\n",
    "There do exist at higher resolution other geographic datasets (e.g., climatic, geological, and topographic) that are intimately related to soil moisture values. SOMOSPIE is meant to be a general-purpose tool for using such datasets to downscale (i.e., increase resolution) satelite-based soil moisture products. This Jupyter Notebook is a result of a collaboration between computer scientists of the Global Computing Laboratory at the Universtiy of Tennessee, Knoxville and soil scientists at the University of Delware (funded by NSF awards #1724843 and #1854312).\n",
    "\n",
    "## Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSN credentials and setup\n",
    "Before running the workflow, specify your access key and secret key in the Pegasus credentials file at ~/.pegasus/credentials.conf with the format below.\n",
    "\n",
    "```\n",
    "[osn]\n",
    "endpoint = https://sdsc.osn.xsede.org\n",
    "\n",
    "[USER@osn]\n",
    "access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "secret_key = abababababababababababababababab\n",
    "```\n",
    "**Note:** Replace USER with your ACCESS username\n",
    "\n",
    "In the following code cell also specify the OSN bucket and ACCESS username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update to a OSN bucket you have access to. For example asc190064-bucket01 \n",
    "osn_bucket=\"BUCKET\" \n",
    "# update to your ACCESS username\n",
    "access_user=\"ACCESS\"\n",
    "\n",
    "!chmod 600 ~/.pegasus/credentials.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters\n",
    "In the code cell bellow specify the inputs to the workflow:\n",
    "* **train_path:** Path to the training file in GeoTIF format.\n",
    "* **eval_paths:** Paths to the evaluation files in GeoTIF format.\n",
    "* **model:** Model to train (knn or rf).\n",
    "* **maxk_maxtree:** Maximum k to try for finding optimal model in case the model is KNN or maximum number of trees to try for finding optimal model in case the model is RF.\n",
    "* **seed:** seed for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/OK_10m/2010_01.tif\"\n",
    "eval_paths = [\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/OK_10m/eval_{0:04d}.tif\".format(i) for i in range(36)]\n",
    "\n",
    "# Remove empty tifs\n",
    "# eval_paths.remove(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/OK_10m/eval_0003.tif\")\n",
    "# eval_paths.remove(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/OK_10m/eval_0004.tif\")\n",
    "# eval_paths.remove(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/OK_10m/eval_0005.tif\")\n",
    "# eval_paths.remove(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/OK_10m/eval_0011.tif\")\n",
    "# eval_paths.remove(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/OK_10m/eval_0017.tif\")\n",
    "\n",
    "model = 'rf' # knn or rf\n",
    "maxk_maxtree = 20 # Depending on choice of model this will set the \n",
    "seed = 1024 # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus logging and properties\n",
    "Some properties for the workflow are specified, such as the data staging configuration to NonShared FileSystem to be able to use OSN for the intermediate and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "\n",
    "# --- Properties ---------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"  \n",
    "# props[\"pegasus.mode\"] = \"tutorial\" # speeds up tutorial workflows - remove for production ones\n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "props[\"pegasus.data.configuration\"] = \"nonsharedfs\"\n",
    "props[\"pegasus.transfer.threads\"] = \"10\"\n",
    "props[\"pegasus.transfer.lite.threads\"] = \"10\"\n",
    "#props[\"pegasus.transfer.bypass.input.staging\"] = \"true\"\n",
    "props[\"pegasus.integrity.checking\"] = \"none\" # temporary, bug\n",
    "props.write() # written to ./pegasus.properties "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replica Catalog\n",
    "The input files to the workflow are specified in the Replica Catalog, specifically the input tiles that Pegasus ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = ReplicaCatalog()\n",
    "\n",
    "train_file = File(os.path.basename(train_path))\n",
    "rc.add_replica(site=\"osn\", lfn=train_file, pfn=train_path)\n",
    "train_aux_file = File(os.path.basename(train_path) + \".aux.xml\")\n",
    "rc.add_replica(site=\"osn\", lfn=train_aux_file, pfn=train_path + \".aux.xml\")\n",
    "\n",
    "eval_files = []\n",
    "eval_aux_files = []\n",
    "for eval_path in eval_paths:\n",
    "    eval_files.append(File(os.path.basename(eval_path)))\n",
    "    eval_aux_files.append(File(os.path.basename(eval_path) + \".aux.xml\"))\n",
    "    rc.add_replica(site=\"osn\", lfn=eval_files[-1], pfn=eval_path)\n",
    "    rc.add_replica(site=\"osn\", lfn=eval_aux_files[-1], pfn=eval_path + \".aux.xml\")\n",
    "\n",
    "rc.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Catalog\n",
    "In this catalog the container in which the workflow will be run is specified along with the scripts that contain each of the functions of the workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Container ----------------------------------------------------------\n",
    "base_container = Container(\n",
    "                  \"base-container\",\n",
    "                  Container.SINGULARITY,\n",
    "                  image=\"docker://olayap/somospie-gdal\")\n",
    "\n",
    "# --- Transformations ----------------------------------------------------------\n",
    "train_model = Transformation(\n",
    "                \"train_model.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/train_model.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_memory=\"1GB\")\n",
    "\n",
    "evaluate_model = Transformation(\n",
    "                \"evaluate_model.py\",\n",
    "                site=\"local\",\n",
    "                pfn=Path(\".\").resolve() / \"code/evaluate_model.py\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_memory=\"125GB\")\n",
    "\n",
    "\n",
    "tc = TransformationCatalog()\\\n",
    "    .add_containers(base_container)\\\n",
    "    .add_transformations(train_model, evaluate_model)\\\n",
    "    .write() # written to ./transformations.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site Catalog\n",
    "Specifies the OSN bucket where the files from the workflow will be stored and the local site where the input files and scripts are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Site Catalog ------------------------------------------------- \n",
    "osn = Site(\"osn\", arch=Arch.X86_64, os_type=OS.LINUX)\n",
    "\n",
    "# create and add a bucket in OSN to use for your workflows\n",
    "osn_shared_scratch_dir = Directory(Directory.SHARED_SCRATCH, path=\"/\" + osn_bucket + \"/SOMOSPIE/work\") \\\n",
    "    .add_file_servers(FileServer(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/SOMOSPIE/work\", Operation.ALL),)\n",
    "osn_shared_storage_dir = Directory(Directory.SHARED_STORAGE, path=\"/\" + osn_bucket + \"/SOMOSPIE/storage\") \\\n",
    "    .add_file_servers(FileServer(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/SOMOSPIE/storage\", Operation.ALL),)\n",
    "osn.add_directories(osn_shared_scratch_dir, osn_shared_storage_dir)\n",
    "\n",
    "# add a local site with an optional job env file to use for compute jobs\n",
    "shared_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "local_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "local = Site(\"local\") \\\n",
    "    .add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL)))\n",
    "\n",
    "#job_env_file = Path(str(BASE_DIR) + \"/../tools/job-env-setup.sh\").resolve()\n",
    "#local.add_pegasus_profile(pegasus_lite_env_source=job_env_file)\n",
    "\n",
    "#condorpool_site = Site(\"condorpool\")\n",
    "#condorpool_site.add_condor_profile(request_cpus=1, request_memory=\"9 GB\", request_disk=\"9 GB\")\n",
    "\n",
    "sc = SiteCatalog()\\\n",
    "   .add_sites(osn, local)\\\n",
    "   .write() # written to ./sites.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "The workflow is specified in the next code cell with the inputs, output and intermediate files. The latter also have specified cleanup jobs by using the argument **stage_out=False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"SOMOSPIE\")\n",
    "\n",
    "model_file = File(\"model.pkl\")\n",
    "scaler_file = File(\"scaler.pkl\")\n",
    "job_train = Job(train_model)\\\n",
    "                .add_args(\"-i\", train_file, \"-o\", model_file, \"-s\", scaler_file, \"-m\", model, \"-k\", maxk_maxtree, \"-t\", maxk_maxtree, \"-e\", seed)\\\n",
    "                .add_inputs(train_file, train_aux_file, bypass_staging=False)\\\n",
    "                .add_outputs(model_file, scaler_file, stage_out=True)\n",
    "wf.add_jobs(job_train)\n",
    "\n",
    "for i, (eval_file, eval_aux_file) in enumerate(zip(eval_files, eval_aux_files)):\n",
    "    prediction_file = File(\"predictions_{0:04d}.tif\".format(i))\n",
    "    job_evaluate = Job(evaluate_model)\\\n",
    "                        .add_args(\"-i\", eval_file, \"-o\", prediction_file, \"-s\", scaler_file, \"-m\", model_file)\\\n",
    "                        .add_inputs(eval_file, eval_aux_file, scaler_file, model_file)\\\n",
    "                        .add_outputs(prediction_file, stage_out=True)\n",
    "    \n",
    "    wf.add_jobs(job_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n",
    "\n",
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and submit the Workflow\n",
    "In this case OSN is specified for data staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(staging_sites={\"condorpool\": \"osn\"}, sites=[\"condorpool\"], output_sites=[\"osn\"], submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the workflow\n",
    "Pegasus returns statistics from the run of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug the workflow\n",
    "In case of failure `wf.analyze()` is helpful to find the cause of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.analyze()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
