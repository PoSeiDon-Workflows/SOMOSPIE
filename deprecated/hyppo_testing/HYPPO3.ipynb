{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code by Travis Johnston, 2017.\n",
    "# Modified by Danny Rorabaugh, 2018.\n",
    "# Copied from HYPPO2.py to parallelize in Jetstream.\n",
    "\n",
    "import argparse, itertools, csv, random\n",
    "import numpy as np\n",
    "from scipy.special import comb # Use comb(n, r, exact=True) to return int instead of float.\n",
    "from functools import reduce\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization.\n",
    "def init_parallel():\n",
    "    print(f\"There are {os.cpu_count()} cores, of which {len(os.sched_getaffinity(0))} are available.\")\n",
    "\n",
    "    # https://docs.python.org/3/library/multiprocessing.html\n",
    "    #import multiprocessing as mp\n",
    "    #def f(x):\n",
    "    #    return x*x*x\n",
    "    #for i in range(7, 9):\n",
    "    #    print(f\"Round {i} begins!\")\n",
    "    #    t0 = time()\n",
    "    #    with mp.Pool() as p:\n",
    "    #        p.map(f, range(10**i))\n",
    "    #    print(time() - t0)\n",
    "    #    t0 = time()\n",
    "    #    [f(x) for x in range(10**i)]\n",
    "    #    print(time() - t0)\n",
    "\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    # https://spark.apache.org/docs/0.9.0/api/pyspark/\n",
    "    global SC\n",
    "    from pyspark import SparkContext as SC\n",
    "    #sc = SC.getOrCreate()\n",
    "    #data = sc.parallelize(range(10))\n",
    "    #print(data.collect())\n",
    "    #sc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function expects a list of coefficients for the polynomial in order: \n",
    "### This function expects the degree of the polynomial (integer).\n",
    "### This function expects a point (list of floats) of where to evaluate the polynomial.\n",
    "### This function returns the value of the polynomial evaluated at the point provided.\n",
    "def evaluate_polynomial(coefficients, degree, point):\n",
    "    if degree == 0:\n",
    "        return coefficients[0]\n",
    "    \n",
    "    monomials = [ reduce(lambda a, b: a*b, x) for x in itertools.combinations_with_replacement([1.0] + point, degree) ]\n",
    "    return sum( [ a[0]*a[1] for a in zip(coefficients, monomials) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### independent_variable_points is a list of settings for the independent variables that were observed.\n",
    "### dependent_variable_values is a list of observed values of the dependent variable.\n",
    "### It is important that for each i the result of independent_variable_points[i] is stored as dependent_variable_values[i].\n",
    "### degree is the degree of the polynomial to build.\n",
    "### This function returns the list of coefficients of the best fit polynomial surface of degree \"degree\".\n",
    "def determine_coefficients(independent_variable_points, dependent_variable_values, degree):\n",
    "    X = []\n",
    "    if degree > 0:\n",
    "        for iv in independent_variable_points:\n",
    "            X.append( [ reduce(lambda a, b: a*b, x) for x in itertools.combinations_with_replacement([1.0] + iv, degree) ] )\n",
    "    else:\n",
    "        X = [ [1] for iv in independent_variable_points ]\n",
    "    X = np.array(X)\n",
    "    Xt = np.transpose(X)\n",
    "    Z = np.array(dependent_variable_values)\n",
    "    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html\n",
    "    # If the following cannot be solved deterministically, the function defaults to using np.linalg.lstsq()\n",
    "    coef = np.linalg.solve( np.dot(Xt, X), np.dot(Xt, Z) )\n",
    "    return list( coef )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data_points is a list of the observed independent variable settings.\n",
    "### specific_points is one chosen setting of the independent variables.\n",
    "### k is the number of nearest neighbors to find.\n",
    "### scale indicates how the coordinates can/should be re-scaled before the distance metric is computed.\n",
    "### For example, if the points are of the form (x, y) and x's are measured in 1's and y's are measured by 100's.\n",
    "### Then, it may be desirable to multiply the x-values by 100 to bring them onto the same scale as the y-values.\n",
    "### To do this, set scale=[100, 1]. The default for CONUS is [1, 2],\n",
    "### since each degree latitude (y step) is about twice as long as each degree longitude (x step).\n",
    "### This function returns a list of indices (in data_points) of the k nearest neighbors.\n",
    "### If specific_point is among the data sampled in data points (distance < error) it is excluded from the neighbors.\n",
    "def indices_of_kNN(data_points, specific_point, k, scale=[1, 2], error=.0001):\n",
    "    \n",
    "    scale = np.array(scale)\n",
    "    scaled_data = [ np.array(x[:2])*scale for x in data_points ]\n",
    "    specific_point = np.array(specific_point[:2])*scale\n",
    "\n",
    "    distances = [ sum( (x - specific_point)**2 ) for x in scaled_data ]\n",
    "    indices = np.argsort( distances, kind='mergesort' )[:k+1]\n",
    "\n",
    "    if distances[indices[0]] < error:\n",
    "        return indices[1:]\n",
    "    else:\n",
    "        return indices[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variant of above for higher dimensional space.\n",
    "### Pre-scaled data assumed.\n",
    "def indices_of_kNN_ndim(data_points, specific_point, k, scale=[]):\n",
    "    if len(data_points)==0:\n",
    "        print(\"Error: data_points empty!\")\n",
    "        return False\n",
    "    if len(data_points[0])!=len(specific_point):\n",
    "        print(\"Error: specific_point not same dim as elements of data_points!\")\n",
    "        return False\n",
    "\n",
    "    if scale:\n",
    "        if len(scale)!=len(specific_point):\n",
    "            print(\"Error: scale specified, but of different length then data!\")\n",
    "            return False\n",
    "        scale = np.array(scale)\n",
    "        data_points = [ np.array(x)*scale for x in data_points ]\n",
    "        specific_point = np.array(specific_point)*scale\n",
    "    \n",
    "    distances = [ sum( (x - specific_point)**2 ) for x in data_points ]\n",
    "    indices = np.argsort( distances, kind='mergesort' )[:k]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the standard deviation of all columns of df\n",
    "def compute_scale(df):\n",
    "    return [col.std(ddof=0) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### independent_data_points is a list of the observed independent variables to build models from.\n",
    "### dependent_data_points is a list of the observed dependent variables (in the same order).\n",
    "### k is the number of folds or partitions to divide the data into.\n",
    "### num_random_partitions is the number of times the data is randomly partitioned (for averaging over many runs).\n",
    "### D is an explicit cap on degree.\n",
    "def kfold_crossvalidation(independent_data_points, dependent_data_points, k, num_random_partitions, D=10):\n",
    "    \n",
    "    n = len(independent_data_points)                ### Number of data points.\n",
    "    dim = len(independent_data_points[0])           ### Dimension of the data.\n",
    "    size_of_smallest_learning_set = (n*k - n)//k    ### Used to constrain degree of polynomial.\n",
    "    degree_cap = 0\n",
    "\n",
    "    # The following guarantees that there is enough data to determine the coefficients uniquely.\n",
    "    while ( comb(degree_cap + dim, dim, exact=True) <= size_of_smallest_learning_set ) and ( degree_cap < D ):\n",
    "        degree_cap += 1\n",
    "\n",
    "    fold_sizes = [n//k]   ### Integer division rounds down.\n",
    "    first_index = [0]     ### Index of first element of the fold in the indices list (below).\n",
    "    for i in range(1, k):\n",
    "        fold_sizes.append( (n - sum(fold_sizes))//(k - i) )\n",
    "        first_index.append( first_index[i - 1] + fold_sizes[i - 1] )\n",
    "    first_index.append(n)\n",
    "\n",
    "    ### A list of 0's of same length as possible degrees.\n",
    "    Total_SSE = [0]*degree_cap\n",
    "    \n",
    "    for iteration in range(num_random_partitions):\n",
    "\n",
    "        ### Randomly partition the data into k sets as equally sized as possible.\n",
    "        indices = list(range(n))\n",
    "        ### Get a new random shuffling of the indices.\n",
    "        random.shuffle(indices)\n",
    "        Folds = [ indices[first_index[fold]:first_index[fold + 1]] for fold in range(k) ]\n",
    "        \n",
    "        for d in range(degree_cap):\n",
    "\n",
    "            ### Build k models of degree d (each model reserves one set as testing set).\n",
    "            for testing_fold in range(k):\n",
    "                testing_independent_data = [ independent_data_points[i] for i in Folds[testing_fold] ]\n",
    "                testing_dependent_data = [ dependent_data_points[i] for i in Folds[testing_fold] ]\n",
    "                \n",
    "                model_independent_data = []\n",
    "                model_dependent_data = []\n",
    "                for fold in range(k):\n",
    "                    if fold != testing_fold:\n",
    "                        model_independent_data += [ independent_data_points[i] for i in Folds[fold] ]\n",
    "                        model_dependent_data += [ dependent_data_points[i] for i in Folds[fold] ]\n",
    "                \n",
    "                ### Get the polynomial built from the model data of degree d.\n",
    "                try:\n",
    "                    coefficients = determine_coefficients( model_independent_data, model_dependent_data, d )\n",
    "                \n",
    "                    ### Predict the testing points and add the error to the Total_SSE[d].\n",
    "                    for x, z in zip(testing_independent_data, testing_dependent_data):\n",
    "                        ### The square of the difference between polynomial prediction and observed value (z) at x.\n",
    "                        Total_SSE[d] += (evaluate_polynomial(coefficients, d, x) - z)**2    \n",
    "                    #print(f\"d: {d}; Total_SSA[d]: {Total_SSE[d]}; \\ncoefficients: {coefficients}\\n\")\n",
    "\n",
    "                except:\n",
    "                    Total_SSE[d] += 99999999999        ### Basically, this d was too big.\n",
    "\n",
    "    ### Return index of minimum Total_SSE.\n",
    "    ### Note: Total_SSE[i] corresponds to polynomial of degree i.\n",
    "    winning_degree = Total_SSE.index(min(Total_SSE))\n",
    "    #print(f\"n: {n}; dim: {dim}; degree_cap: {degree_cap}; winning_degree: {winning_degree}; \\nTotal_SSE: {Total_SSE}\\n\")\n",
    "    \n",
    "    return winning_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ideal for small sample sizes\n",
    "def leave_one_out_crossvalidation(independent_data_points, dependent_data_points):\n",
    "    return kfold_crossvalidation(independent_data_points, dependent_data_points, len(independent_data_points), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main function for a single data point.\n",
    "### This is the function that will be called independently many time.\n",
    "### If this can be run on every element of a Spark RDD, we're golden.\n",
    "def model_at_point(x, Independent_Data, Dependent_Data, K, model=\"HYPPO\"):#, ndim=True): \n",
    "\n",
    "            ### Find Nearest neighbors\n",
    "            indices_of_nearest_neighbors = indices_of_kNN_ndim(Independent_Data, x, K)#, scale=(not ndim)*[1,2])\n",
    "\n",
    "            ### Select the data associated with the nearest neighbors for use with modeling\n",
    "            selected_independent_data = [ Independent_Data[i] for i in indices_of_nearest_neighbors ]\n",
    "            selected_dependent_data = [ Dependent_Data[i] for i in indices_of_nearest_neighbors ]\n",
    "\n",
    "            ### Determine the best polynomial degree\n",
    "            if model == \"KNN\":\n",
    "                ### Setting the degree to 0 forces us to just average the nearest neighbors.\n",
    "                ### This is exactly kNN (a degree 0 polynomial).\n",
    "                degree = 0\n",
    "        \n",
    "            elif model == \"SBM\":\n",
    "                degree = kfold_crossvalidation(selected_independent_data, selected_dependent_data, 10, 10)\n",
    "            \n",
    "            elif model==\"HYPPO\":\n",
    "                degree = leave_one_out_crossvalidation(selected_independent_data, selected_dependent_data)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"\\\"{model}\\\" is not a valid model.\")\n",
    "\n",
    "            ### Compute the coefficients of the \"best\" polynomial of degree degree.\n",
    "            coefficients = determine_coefficients(selected_independent_data, selected_dependent_data, degree)\n",
    "\n",
    "            ### Using the surface, predict the value of the point.\n",
    "            z = evaluate_polynomial(coefficients, degree, x)\n",
    "            \n",
    "            #if degree > 0:\n",
    "            #    print(f\"x: {x}; \\nindices_of_nearest_neighbors: {indices_of_nearest_neighbors}; \\ndegree: {degree}; coefficients: {coefficients}; \\nz: {z}\\n\")\n",
    "            return (z,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input1 and input2 are arrays or ndarrays.\n",
    "### Columns index 0 and 1 of input1 and input2 are the x/y-coordinates.\n",
    "### input1 should have 1 more column than input2, the column with the dependent variable.\n",
    "### depIndex is the index of the dependent variable column in input1.\n",
    "### model is one of [\"HYPPO\", \"KNN\", \"SBM\"].\n",
    "### Implementations of HYPPO and SBM are not well-suited for high dimensional data.\n",
    "### k is the number of nearest neighbors for HYPPO or KNN (is overridden for SBM).\n",
    "def main(input1, input2, depIndex=2, model=\"HYPPO\", k=6, indepStart=0, indepCount=2, scale=[], parallel=False):\n",
    "\n",
    "    Independent_Data = []\n",
    "    Dependent_Data = []\n",
    "    for line in input1:\n",
    "        numbers = list(line)\n",
    "        Dependent_Data.append(numbers.pop(depIndex))\n",
    "        Independent_Data.append(np.array(numbers[indepStart:indepStart+indepCount]))\n",
    "        #Coordinate_Data.append(np.array(numbers[:2]))\n",
    "\n",
    "    if scale:\n",
    "        if len(scale)!=indepCount:\n",
    "            print(\"Error: scale was specified, but isn't the same length as the sepcified number of independent variables!\")\n",
    "        scale = np.array(scale)\n",
    "    else:\n",
    "        scale = 1/np.std(Independent_Data, axis=0)\n",
    "    print(scale)\n",
    "\n",
    "    print(f\"Dependent_Data is an array of length {len(Dependent_Data)} with first elements:\\n{Dependent_Data[:5]}\\n\")\n",
    "    print(f\"Independent_Data is a length-{len(Independent_Data)} array of arrays with first element:\\n{Independent_Data[0]}\\n\")    \n",
    "    \n",
    "    Independent_Data = [row*scale for row in Independent_Data]\n",
    "    print(f\"Independent_Data post-scaling is an array of arrays with first element:\\n{Independent_Data[0]}\\n\")    \n",
    " \n",
    "    # Set K, the number of nearest neighbors to use when building the model.\n",
    "    if model == \"SBM\":\n",
    "        K = len(Dependent_Data) - 1\n",
    "    else:\n",
    "        K = k\n",
    "    print(f\"Each local model will be generated with {K} nearest neighbors.\\n\")\n",
    "\n",
    "    t0 = time()\n",
    "    \n",
    "    if parallel:\n",
    "        \n",
    "        def MaP(x):\n",
    "            a = x[0]\n",
    "            b = x[1]\n",
    "            x = np.array(x[indepStart:indepStart+indepCount])*scale\n",
    "            (z,d) = model_at_point(x, Independent_Data, Dependent_Data, K, model)\n",
    "            return [a, b, z, d]\n",
    "        '''\n",
    "        if indepStart>0 or indepCount>2:\n",
    "            def MaP(x):\n",
    "                a = x[0]\n",
    "                b = x[1]\n",
    "                x = np.array(x[indepStart:indepStart+indepCount])*scale\n",
    "                (z,d) = model_at_point(x, Independent_Data, Dependent_Data, K, model, ndim=True)\n",
    "                return [a, b, z, d]\n",
    "        else:\n",
    "            def MaP(x):\n",
    "                a = x[0]\n",
    "                b = x[1]\n",
    "                (z,d) = model_at_point(x[:2], Independent_Data, Dependent_Data, K, model, ndim=False)\n",
    "                return [a, b, z, d]\n",
    "        '''\n",
    "        # The following results in:\n",
    "        # AttributeError: Can't pickle local object 'main.<locals>.MaP'\n",
    "        #with mp.Pool() as p:\n",
    "        #    output = p.map(MaP, input2)\n",
    "            \n",
    "        init_parallel()\n",
    "        sc = SC.getOrCreate()\n",
    "        data = sc.parallelize(input2)\n",
    "        data = data.map(MaP)\n",
    "        output = data.collect()\n",
    "        sc.stop()\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        output = []\n",
    "    \n",
    "        for x in input2:\n",
    "            a = x[0]\n",
    "            b = x[1]\n",
    "            if spatialVars>2:\n",
    "                x = np.array(x[:spatialVars])/scale\n",
    "\n",
    "            #print(f\"x: {x}; K: {K}; model: {model}\")\n",
    "            (z,d) = model_at_point(x, Independent_Data, Dependent_Data, K, model, spatialVars) \n",
    "\n",
    "            output.append([a, b, z, d])\n",
    "    \n",
    "    print(f\"It took {time() - t0} seconds to perform model_at_point on all the evaluation points.\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "38 lines of original data have been loaded from test.csv.\n",
      "\n",
      "\n",
      "43342 lines of evaluation data have been loaded from eval.csv.\n",
      "\n",
      "[1. 2.]\n",
      "Dependent_Data is an array of length 38 with first elements:\n",
      "[0.21897954742113698, 0.23551440735657997, 0.258084189891815, 0.23709106507400698, 0.260583707193534]\n",
      "\n",
      "Independent_Data is a length-38 array of arrays with first element:\n",
      "[-75.875  39.125]\n",
      "\n",
      "Independent_Data post-scaling is an array of arrays with first element:\n",
      "[-75.875  78.25 ]\n",
      "\n",
      "Each local model will be generated with 6 nearest neighbors.\n",
      "\n",
      "There are 24 cores, of which 24 are available.\n",
      "It took 7.864973783493042 seconds to perform model_at_point on all the evaluation points.\n"
     ]
    }
   ],
   "source": [
    "args_fileName = \"test.csv\"\n",
    "args_delimiter = \",\"\n",
    "args_headerRows = 1\n",
    "args_eval = \"eval.csv\"\n",
    "args_depIndex = 2\n",
    "args_model = \"HYPPO\"\n",
    "args_k = 6\n",
    "args_variables = 2\n",
    "args_skipVars = 0\n",
    "args_scale = '1,2'\n",
    "args_out = f\"output{args_variables}d-{args_k}.csv\"\n",
    "args_parallel = True\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":    \n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    parser.add_argument( \"fileName\", help=\"The path to the csv file containing the training data.\")\n",
    "#    parser.add_argument( \"-m\", \"--model\", help=\"The type of model to build.\", choices=[\"HYPPO\", \"KNN\", \"SBM\"], default=\"HYPPO\")\n",
    "#    parser.add_argument( \"-k\", \"--k\", help=\"The number of nearest neighbors to use for either the KNN or HYPPO model.\", type=int, default=6)\n",
    "#    parser.add_argument( \"-e\", \"--eval\", help=\"Name of file where the evaluation points are stored.\")\n",
    "#    parser.add_argument( \"-o\", \"--out\", help=\"Name of file where prediction is to be stored.\")\n",
    "#    parser.add_argument( \"-i\", \"--depIndex\", help=\"Index of column in fileName with dependent variable to be tested for building a model.\", type=int, default=2)\n",
    "#    parser.add_argument( \"-r\", \"--headerRows\", help=\"Number of rows to ignore, being header row(s).\", type=int, default=1)\n",
    "#    parser.add_argument( \"-d\", \"--delimiter\", help=\"Delimiter of fileName and eval.\", default=\",\")\n",
    "#    parser.add_argument( \"-v\", \"--variables\", help=\"Number of independent variables to use; if unspecified, will use two columns in the file.\", type=int, default=2)\n",
    "#    parser.add_argument( \"-s\", \"--skipVars\", help=\"Number of independent variables to skip; e.g., 2 if you don't wish to use lon/lat.\", type=int, default=0)\n",
    "#    parser.add_argument( \"-S\", \"--scale\", help=\"Specify the scale to multiply your independent variables by; for example -s0 -v2 -S1,2.\")\n",
    "#    parser.add_argument( \"-p\", \"--parallel\", help=\"1 to run in parallel with Spark; 0 otherwise.\", type=int, default=0)\n",
    "#    args=parser.parse_args()\n",
    "#\n",
    "#    ### args.fileName contains the data from which to build the model.\n",
    "#    ### It is expected that the file be comma separated and have a header row.\n",
    "#    ### Default format is x, y, z, c1, ..., cm.\n",
    "#    ### Where x and y are geographic coordinates, z is the observed dependent variable,  \n",
    "#    ### and c1, ..., cm are additional independent variables.\n",
    "#    ### args.eval should be the same format, but lacking the z column.\n",
    "\n",
    "if args_scale:\n",
    "    scale = [float(s) for s in args_scale.split(',')]\n",
    "else:\n",
    "    scale = []\n",
    "\n",
    "original_values = np.loadtxt(args_fileName, delimiter=args_delimiter, skiprows=args_headerRows)\n",
    "print(f\"\\n{len(original_values)} lines of original data have been loaded from {args_fileName}.\\n\")\n",
    "\n",
    "values_to_model = np.loadtxt(args_eval, delimiter=args_delimiter, skiprows=args_headerRows)\n",
    "print(f\"\\n{len(values_to_model)} lines of evaluation data have been loaded from {args_eval}.\\n\")\n",
    "\n",
    "output = main(original_values, values_to_model, depIndex=args_depIndex, model=args_model, \n",
    "              k=args_k, indepStart=args_skipVars, indepCount=args_variables, parallel=args_parallel, scale=scale)\n",
    "\n",
    "np.savetxt(args_out, output, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " [2, 4, 7, 11, 16, 22, 29, 37, 46, 56, 67],\n",
       " [2, 5, 11, 21, 36, 57, 85, 121, 166, 221, 287],\n",
       " [2, 6, 16, 36, 71, 127, 211, 331, 496, 716, 1002]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nnnn[deg][dim] is number of nearest neighbors needed \n",
    "# for hyppo to test up to degree deg when there are dim independent variables.\n",
    "nnnn = [[1 + comb(deg + dim, deg, exact=True) for dim in range(11)] for deg in range(5)]\n",
    "nnnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
